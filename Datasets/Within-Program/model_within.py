# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyvcVP-AfeJOt1me3RDIHhCHRTL90C8g

## **Tensorflow version 2.2.0**
"""

import tensorflow as tf
import numpy as np
import tensorflow_datasets as tfds
import pickle

import sklearn.decomposition
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

"""##**Getting the dataset **"""

path = "LSTM_EXT.csv"
dataframe = pd.read_csv(path)

dataframe.head()

dataframe.info()

dataframe.shape

"""Let's obtain the text and labels for the training and test data:"""

data_x = np.asarray(dataframe['text'])
data_y = np.asarray(dataframe['label'])

for t, c in zip(data_x[:5], data_y[:5]):
    print('Text:\n{}\nClass: {}'.format(t,c))

"""## **Tokenization**"""

# We set the num_words = 2000 if we are performing a LSTM-APS transformation, else we leave num_words = None
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 2000)
tokenizer.fit_on_texts(data_x)

# We generate a tokenizer for further implementations
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""Now that we have the tokenizer, we can use it to transform the textual code into sequences of token indexes:"""

data_x_seqs = tokenizer.texts_to_sequences(data_x)

for t in zip(data_x_seqs[:5]):
    print('Text:\n{}'.format(t))

"""Not all code snippets have the same number of words:"""

print('Min length: {}'.format(min([len(seq) for seq in data_x_seqs])))
print('Max length: {}'.format(max([len(seq) for seq in data_x_seqs])))

"""However, all the examples in a training batch must have the same length. A possible approach is to truncate the sequences to a maximum number of words and add padding to those which are shorter than that limit. We can do that using the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) method:"""

data_x = tf.keras.preprocessing.sequence.pad_sequences(data_x_seqs, maxlen=8500)

"""##**Model**##"""

data_rnn_model = tf.keras.Sequential(name='data_rnn')
data_rnn_model.add(tf.keras.layers.Embedding(tokenizer.num_words, 128, name='embedding'))
data_rnn_model.add(tf.keras.layers.LSTM(128, name='recurrent'))
data_rnn_model.add(tf.keras.layers.Dense(2, activation='softmax', name='output'))
data_rnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
data_rnn_model.summary()

"""Now we can train the model.
We will use **5-fold cross-validation** for consistency
"""

average_acc = 0
average_loss = 0
average_prec = 0
average_rec = 0

kFold = KFold(n_splits=5, shuffle = True, random_state=5)

for train_index,test_index in kFold.split(data_x):
  data_train_x, data_test_x = data_x[train_index], data_x[test_index]
  data_train_y, data_test_y = data_y[train_index], data_y[test_index]

  earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)
  checkpoint = tf.keras.callbacks.ModelCheckpoint('data_rnn_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)
  data_rnn_model_train = data_rnn_model.fit(data_train_x, data_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=20, batch_size=4)

  data_rnn_model.load_weights('data_rnn_best.h5')
  loss, acc= data_rnn_model.evaluate(data_test_x, data_test_y)

  # predict classes for test set
  yhat_classes = np.argmax(data_rnn_model.predict(data_test_x), axis=-1)
  # accuracy: (tp + tn) / (p + n)
  accuracy = accuracy_score(data_test_y, yhat_classes)
  # precision tp / (tp + fp)
  prec = precision_score(data_test_y, yhat_classes)
  # recall: tp / (tp + fn)
  rec = recall_score(data_test_y, yhat_classes)

  average_acc = average_acc + acc
  average_loss = average_loss + loss
  average_prec = average_prec + prec
  average_rec = average_rec + rec

print('Accuracy: {}'.format(average_acc / 5 ))
print('Loss: {}'.format(average_loss / 5 ))
print('Precision: {}'.format(average_prec/ 5 ))
print('Recall: {}'.format(average_rec / 5 ))

#Optional to save the model
#model.save('path/to/location')

