# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyvcVP-AfeJOt1me3RDIHhCHRTL90C8g

## **Tensorflow version 2.2.0**
"""

import tensorflow as tf
import numpy as np
import pickle

import sklearn.decomposition
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

"""##**Getting the dataset**"""

path1 = "Avrora.csv"
path2 = "Joda.csv"
path3 = "Jython.csv"
path4 = "Tomcat.csv"
path5 = "Xalan-j.csv"

dataframe1 = pd.read_csv(path1)
dataframe2 = pd.read_csv(path2)
dataframe3 = pd.read_csv(path3)
dataframe4 = pd.read_csv(path4)
dataframe5 = pd.read_csv(path5)

# In this case we want to hold-out dataframe 5

dataframe = dataframe1.append(dataframe2)
dataframe = dataframe.append(dataframe3)
dataframe = dataframe.append(dataframe4)
# dataframe = dataframe.append(dataframe5) # So we don't put it in the training set

dataframe.head()
dataframe.info()
dataframe.shape


data_x = np.asarray(dataframe['text'])
data_y = np.asarray(dataframe['label'])

data_test_x = np.asarray(dataframe5['text'])   #We define the test set as dataframe 5
data_test_y = np.asarray(dataframe5['label'])  #We define the test set as dataframe 5

for t, c in zip(data_x[:5], data_y[:5]):
    print('Text:\n{}\nClass: {}'.format(t,c))

"""## **Tokenization**"""

# We set the num_words = 2000 if we are performing a LSTM-APS transformation, else we leave num_words = None
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 2000)
tokenizer.fit_on_texts(data_x)

# We generate a tokenizer for further implementations
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""Now that we have the tokenizer, we can use it to transform the textual code into sequences of token indexes:"""

data_x_seqs = tokenizer.texts_to_sequences(data_x)
data_test_x_seqs = tokenizer.texts_to_sequences(data_test_x)


"""However, all the examples in a training batch must have the same length. A possible approach is to truncate the sequences to a maximum number of words and add padding to those which are shorter than that limit. We can do that using the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) method:"""

data_x = tf.keras.preprocessing.sequence.pad_sequences(data_x_seqs, maxlen=8500)
data_test_x = tf.keras.preprocessing.sequence.pad_sequences(data_test_x_seqs, maxlen=8500)


data_rnn_model = tf.keras.Sequential(name='data_rnn')
data_rnn_model.add(tf.keras.layers.Embedding(tokenizer.num_words, 128, name='embedding'))
data_rnn_model.add(tf.keras.layers.LSTM(128, name='recurrent'))
data_rnn_model.add(tf.keras.layers.Dense(2, activation='softmax', name='output'))
data_rnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
data_rnn_model.summary()

data_train_x = data_x
data_train_y = data_y 

average_acc = 0
average_loss = 0
average_prec = 0
average_rec = 0

for i in range(5):

    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1)
    checkpoint = tf.keras.callbacks.ModelCheckpoint('data_rnn_best.h5', monitor='val_acc', verbose=1, save_best_only=True)
    data_rnn_model_train = data_rnn_model.fit(data_train_x, data_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=20, batch_size=4)

    data_rnn_model.load_weights('data_rnn_best.h5')
    loss, acc = data_rnn_model.evaluate(data_test_x, data_test_y)

    # predict classes for test set
    yhat_classes = np.argmax(data_rnn_model.predict(data_test_x), axis=-1)
    # accuracy: (tp + tn) / (p + n)
    accuracy = accuracy_score(data_test_y, yhat_classes)
    # precision tp / (tp + fp)
    prec = precision_score(data_test_y, yhat_classes)
    # recall: tp / (tp + fn)
    rec = recall_score(data_test_y, yhat_classes)

    average_acc = average_acc + acc
    average_loss = average_loss + loss
    average_prec = average_prec + prec
    average_rec = average_rec + rec

f = open("Cdata.txt", "w")                          # We write it to a file
f.write('Accuracy: {}'.format(average_acc / 5 ))
f.write('Loss: {}'.format(average_loss / 5 ))
f.write('Precision: {}'.format(average_prec/ 5 ))
f.write('Recall: {}'.format(average_rec / 5 ))
f.close()


